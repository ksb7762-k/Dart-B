## 11. 데이터 전처리와 파생변수 생성 



## 11.1 결측값 처리 

 결측값 처리 방법 결정전에 

 1. 결측값의 비율
 2. 한 변수에 결측값이 몰려 있지는 않은지  
 3. 빈 문자열이 입력되어 있어서 결측값으로 인식되지 않는 지 **파악**

**결측값 처리 방법**   


-  **결측값이 심하게 많으면** **변수 자체를 제거**하거나 **결측값이 포홤된 행을 제거** 
-  평균, 최빈값, 중앙값, 최댓값, 최솟값 등으로 대치   

-  회귀식을 이용하여 결측값을 추정하는 **최귀 대치법** , 변수의 분산을 과소추정하는 문제를 해결하기 위해 **확률적 회귀대치법**등을 사용하기도 함   

-  최근 많이 사용되는 방법은 **다중 대치법** 이는 단순대치를 여러 번 수행하여 n개의 가상적 데이터를 생성하여 이들의 평균으로 결측값을 대체하는 방법    
- 다중대치는 sklearn의 impute 패키지 활용 

**bfill(Backward Fill)과 pad(Forward Fill)**

| **옵션** | **방향** | **채우는 기준** | **주요 특징** |
|-----------|---------|-----------------|----------------|
| **bfill** | **뒤로 채움** | **다음 유효한 값** | 미래 데이터 사용, 시계열에 주의 필요 |
| **pad**   | **앞으로 채움** | **이전 유효한 값** | 과거 데이터 사용, 일반적으로 더 직관적 |

## 11.2 이상치 처리 

- 해당 값을 결측값으로 대체한 다음 결측값 처리를 하거나, 아예 해당 이상치를 제거 하는 것이 가장 간단

- 하지만 그렇게 하면 추정치의 분산은 감소하지만 실제값을 과장하여 편향 발생

- 그래서 하한 값과 상한 값을 정한 후 하한 값보다 작으면 하한 값으로 대체하고 상한 값보다 크면 상한 값으로 대체하는 **관측값 변경**이나 이상치의 영향을 감소시키는 가중치를 주는 **가중치 고정** 방법을 사용 

### 이상치 기준 

- 먼저 **통계치를 통한 무조건적인 이상치 탐색은 위험**할 수 있음 

- 효과적인 이상치 탐색을 위해서는 해당 데이터 변수들의 의미와 비즈니스 도메인을 먼저 이해하고 이상치가 생긴 원인을 논리적으로 생각해야함 
- 또한 분석 도메인에 따라 이상치가 중요한 분석 요인일 수 있음 
- 박스플롯 상에서 분류된 극단치를 그대로 선정 (IQR)
- 임의로 허용범위를 설정하여 이를 벗어나는 값을 이상치로 정의 (Z-SCORE)
- 분포가 비대칭인 경우 -n과 +n 표준편차 값을 다르게 설정하기도 함 
- 좀 더 정교하게 들어가면 이상치에 보다 강건한 중위수와 중위수 절대 편차(MAD)를 사용하기도 함

## 11.3 변수 구간화 (Binning) 

- 방법은 생략 
- 변숫값이 효과적으로 구간화됐는지는 WOE(Wegiht of Evidence)값, IV(Information Value)값 등을 통해 측정  

### WOE 

이진 분류 문제에서 범주형 또는 연속형 변수를 구간으로 나누어 변수의 예측력을 평가하는 데 사용되는 통계적 지표   



![](/image_STA/6-2.PNG)  



![](/image_STA/6-3.PNG)  



![](/image_STA/6-1.PNG)  



IV수치가 높을수록 종속변수의 True와 False를 잘 구분할 수 있는 정보량이 많다는 뜻 ! 


일반적으로 **IV 값**이 0.3보다 큰 경우는 예측력이 우수한 변수인 것으로 판단


## 11.4  데이터 표준화와 정규화 스케일링 


**RobustScaler**
- 데이터의 중앙값(Q2)을 0으로 잡고 Q1(25%)과 Q3(75%) 사분위수와의 IQR차이를 1이 되도록 하는 스케일링 기법 

## 11.5 파생변수 

- 파생변수는 기존의 변수를 활용해서 만든 변수이기 때문에 다중공선성의 문제가 발생할 가능성이 높음.   
- 그렇기 때문에 파생변수를 만든 다음에는 상관분석을 통해 변수간의 상관성을 확인해야 함 

~~11.6 슬라이딩 윈도우 데이터 가공~~

## 11.7 범주형 변수의 가변수 처리 

**변수 간의 독립성을 위해 하나의 범주 가변수를 제거해 주는 것이 중요**

## 11.8 클래스 불균형 

**일반적으로 기계학습 분류 모델은 적은 비중의 클래스 든 큰 비중의 클래스 든 중요도에 차별을 두지 않고 전체적으로 분류를 잘 하도록 학습됨** 

따라서 데이터 불균형이 심하면 우리가 실제로 원하는 성능의 분류 모델을 만들 수 없음  

### 해결방법 
1. **가중치 밸런싱 (Weight balancing)** : 중요도가 높은 클래스를 잘못 분류하면 더 큰 
손실을 계산하도록 조정  

2. **언더샘플링** 
- 랜덤 언더샘플링 : 작은 비중의 클래스와 비율이 유사해질 때까지 무작위로 큰 비중의 클래스의 관측치를 제거하는 단순한 방식  
- EasyEnsemble

- CNN(Condensed Nearest Neighbor)
3. **오버샘플림**
- 랜덤 오버샘플링 : 작은 클래스의 관측치를 단순히 무작위로 선택하여 반복 추출하는 방식 => 과적합 발생 가능 
- SMOTE (Syntehtic Minority Over-Sampling Technique) : KNN기법 사용 
- ADASYN (Adaptive Synthetic Sampling Approach) : SMOTE 상위버전 

- 오버샘플링을 적용할 때에는 먼저 학습 셋과 데이터 셋을 분리한 다음에 적용해야함
- 그렇지 않으면 학습 셋과 테스트 셋에 동일한 데이터가 들어가서 과적합을 유발
- 학습된 모델의 예측력을 검정할 때는 사용하는 테스트 셋에는 오버샘플링을 적용하지 않은 순수한 데이터를 사용해야 함  
- 그리고 오버샘플링이나 언더샘플링을 적욯했을 때는 그렇지 않은 경우보다 예측 성능의 편차가 증가함 => 여러 번 테스트를 하여 표준편차와 같은 평가 척도의 변동에 대한 정보를 같이 표기하는 것이 좋음 

~~11.9 데이터 거리 측정~~